---
title: "Rapport Data Science"
author: "Mehdi KHAIROUN, William LAURENT, Pierre MARJOLLET, Afaf TABAMMOUT"
date: "9 avril 2018"
output:
  word_document:
    df_print: paged
---

```{r setup, include=FALSE}
library(dplyr)
library(magrittr)
library(rpart)
library(rpart.plot)
library(caret)
library(xgboost)
library(fExtremes)
library(gbm)
library(Matrix)
load("data/trainandtest.rda")
knitr::opts_chunk$set(echo = TRUE)
```

<!-- ![](images/logoisfa.jpg)  -->

***

# I. Matériel et pré-processing

## 1. Jeu de données

Les données étudiées sont deux bases issues de la librairie CASdatasets (CAS pour Computational Actuarial Sciences). **freMTPL2freq** et **freMTPL2sev** sont deux bases autos.

Brièvement **freMTPL2freq** est une base de fréquence, nous donnant des caractéristiques pour des polices d'assurances données et leurs nombre de sinitres respectifs.**Cette base dispose de 12 variables et 678 013 individus**, comprenant :

* **IDpol** [Clé primaire : Entier de 1 à 6 114 330] : l'identifiant de la police d'assurance
* **ClaimNb** [Entier de 0 à 16] : le nombre de sinistres de la police
* **Exposure** [Réel de 0 à 1] : l'exposition - la durée de vie de la police. De 0 pour une personne non assuré à 1 pour une personne assurée toute l'année
* **Area** [Modalité de A à F] : indication sur la zone d'apparition du sinistre
* **VehPower** [Entier de 5 à 15] : puissance du véhicule
* **VehAge** [Entier de 0 a 100] : age du véhicule
* **DrivAge** [Entier de 18 à 100] : age du conducteur
* **BonusMalus** [Entier de 50 à 230] : bonus/malus du conducteur. Plus le score est haut, moins l'assuré est sanctionné par son passé
* **VehBrabd** [Modalité de B1 à B12] : marque du véhicule
* **VehGas** [Booléent : 0 ou 1] : véhicule diéserl ou non
* **Density** [Entier de 1 à 1658] : densité de population de la police
* **Region** [24 modalité de R11 à R94] : code région de la police

Enfin, **freMTPLsev** est une base sévèrité, avec simplement un identifiant et une valeur de sinistre. **Cette base comprends 2 variables et 26 639 individus**:

* **IDpol** [Entier de 139 à 6113971] : l'identifiant de la police d'assurance
* **ClaimAmount** [Réel de 1 à 4 075 401] : coût du sinistre

## 2. Pré-processing

Nous avons calculé la moyenne des coûts des sinistres par polices sur la base **freMTPLsev**. Le but étant de savoir quelle est la moyenne des sinistres d'une police.

Nous avons ensuite fusionné les bases **freMTPLsev** et **freMTPLfreq** par **IDpol** maintenant unique sur les deux bases.

Il vient ensuite une base sur laquelle nous retirons les sinistres graves (ceux supérieures à $20 000$). Ce seuil a été déterminé rapidement par l'étude d'un mean-excess plot. Ces quelques sinistres biaisais fortement nos modélisations et seront répartie comme une sur-prime sur l'ensemble de nos assurés.

Nous basons nos modélisations sur seulement $70%$ de nos données pour se laisser une bonne marge de test. Ensuite, pour les procédures plus lourde (typiquement le gradian boosting et les réseaux de neuronnes) les derniers modèles retenus ont été entraîner sur un sous jeux de cette base d'entraînement.

Pour les modélisations en sévérité, nous avons biensûre enlevé les polices ne présentant aucun sinistre.

## 3. R et ses extentions

Dans ce projet nous utilisons le logiciel opensource R et nous utilisons son extention RMarkdown pour la rédaction de ce rapport.

Pour pouvoir mener à bien ce projet, nous avons munit R des librairies suivantes :

* **caret** : **C**lassification **a**nd **re**gression **t**raining, utilisé ici pour utiliser nos algorithmes aisément sur des grilles de paramètre de tuning. Propose également une parrallélisation de ses algorithmes. **caret** dispose de tous les algorithmes nécessaire au bon déroulement de ce projet et pourrait nous suffire ici.
* **doParallel** : s'accouple à **caret** pour accéder à la parrallélisation des algorithmes et ainsi économiser quelques précieuses heures de run.
* **rpart** : permets de dresser des arbres (ici des arbres de régressions).
* **rpart.plot** : utilisé ici pour afficher des graph d'arbres de régressions.
* **magrittr** : donner au code plus de lisibilité.
* **etc**

# II. Arbres de régression

# III. Forêts aléatoires

## 1. Présentation du procédé

Avant de parler de random forests (forêts aléatoires) parlons de Bagging. Le bagging de Bootstrap Aggregating est simplement un bootstrap sur des modélisations données. C'est une méthode d'aggrégation de modèles. Nous avons fait poussé des arbres dans la partie précédente; nous allons ici en faire poussé beaucoup et les aggrégés pour fabriquer des forêts!

La méthode de random forest que nous allons exploiter ici est celle qui consiste à faire un Bagging sur des arbres de régressions maximaux. On va faire pousser une forêt de $ntree$ arbres et chacun de ces arbres aura un choix aléatoire de $mtry$ variables a chaque noeud. Le seul hyperparamètre qui nous intéressera par la suite est $mtry$. $ntree$ lui est un paramètre sur la convergence des données, il va de sois qu'avec un grand $ntree$ nous convergerons que mieux. $mtry$ lui :

* **Pour des petites valeurs** : les noeuds disposerons que de peu de variable pour se former, l'arbre peu être unitairement pauvre en puissance prédictive mais la correlation entre les arbres est minimisée.
* **Pour des grandes valeurs** : les noeuds disposerons de beaucoup de variable pour se former, l'arbre peu être riche en puissance prédictive (comme un CART maximal) mais la correlation entre les arbres sera forte
* Ainsi, un $mtry$ sera déterminé par grille de paramêtres.

## 2. Modèlisation du coût moyen

### A. Recherche de $mtry$

Nous avons fais tourner plusieurs forêts pour obtenir un $mtry$ satisfaisant :

```{r plot rf cout mtry, echo = FALSE, fig.height=7, fig.width=10}

# le modèle est trop lourd, on se contentera de la restitution des résultats
# load("data/cout_model_rf_runing_time.rda")

load("data/cout_model_rf_runing_time_results.rda")

plot(
  RMSE_cout_model_rf_test
  , main = "RMSE\nModèles sur la sévérité"
  , xlab = "mtry"
  , ylab = "rmse"
  , col = "green"
  , lwd = 2
  , ylim = range(range(RMSE_cout_model_rf_test), range(RMSE_cout_model_rf_train))
  , sub = "25 échantillons bootstrap sur 70% de la donnée, 100 arbres par forêt"
  , type = "l"
  )

lines(RMSE_cout_model_rf_train, col = "blue", lwd = 2)

legend("topleft", legend=c("Sur base d'entrainement", "Sur base de test"),
       col=c("blue", "green"), lty=1:1, cex=1)

```

Sur ce graph on trouve que la région des bons $mtry$ se trouve entre $1$ et $5$. Dans la pratique, pour un problème de régression, randomForest conseillera de prendre la racine du nombre de variable (vers $6$, $7$ ici).

On va refaire pousser des fôrets cette fois-ci en poussant le nombre d'arbre à $1000$ par forêt pour ensuite séléctionner un modèle final

## B. Comparaison des forêts retenues

Enfin,après avoir poussé le nombre d'arbre par forêts :

```{r final cout graph, echo = FALSE, fig.height=7, fig.width=10}

load("data/cout_model_rf_final_runing_time_results.rda")

plot(
  RMSE_cout_model_rf_test_final
  , main = "RMSE\nModèles finaux sur la sévérité"
  , xlab = "mtry"
  , ylab = "rmse"
  , col = "green"
  , lwd = 2
  , ylim = range(range(RMSE_cout_model_rf_test_final), range(RMSE_cout_model_rf_train_final))
  , sub = "10 échantillons bootstrap sur 70% de la donnée, 1000 arbres par forêt"
  , type = "l"
  )

lines(RMSE_cout_model_rf_train_final, col = "blue", lwd = 2)

legend("center", legend=c("Sur base d'entrainement", "Sur base de test"),
       col=c("blue", "green"), lty=1:1, cex=1)


```

```{r final cout table, echo = FALSE}

load("data/cout_model_rf_final_runing_time_results.rda")

best_rf_model_cout <- data.frame(matrix(0, 5, 4), row.names = c("mtry = 1", "mtry = 2", "mtry = 3", "mtry = 4", "mtry = 5"))

colnames(best_rf_model_cout) <- c("Proportion de donnée","RMSE entrainement", "RMSE test", "Biais test")

best_rf_model_cout[,1] <- c("70%", "70%", "70%", "70%", "70%")
best_rf_model_cout[,2] <- RMSE_cout_model_rf_train_final
best_rf_model_cout[,3] <- RMSE_cout_model_rf_test_final
best_rf_model_cout[,4] <- biais_cout_finaux

print(best_rf_model_cout)

```

Et le meilleur modèle est celui avec $mtry=2$ même s'il est plutôt biaisé par rapport aux autres, la RMSE est meilleure donc on se trompe moins pour chacune des polies mais en moyenne on se trompe plus.


## 3. Modélisation de la fréquence

### A. Recherche de $mtry$

Comme pour la partie précédente, nous avons tourner nos algorithme sur sur la sévérité pour obtenir une idée d'un bon $mtry$

```{r comparaison foret, echo = FALSE, fig.height=7, fig.width=10}

load("data/1_20mtryonfreq_results.rda")

plot(
  RMSE_predictions_freq_model_rf
  , main = "RMSE\nModèles sur la sévérité"
  , xlab = "mtry"
  , ylab = "rmse"
  , col = "green"
  , lwd = 2
  , ylim = range(range(RMSE_predictions_freq_model_rf), range(RMSE_training_freq_model_rf))
  , sub = "25 échantillons bootstrap sur 70% de la donnée, 100 arbres par forêt"
  , type = "l"
  )

lines(RMSE_training_freq_model_rf, col = "blue", lwd = 2)

legend("topleft", legend=c("Sur base d'entrainement", "Sur base de test"),
       col=c("blue", "green"), lty=1:1, cex=1)

```

On retient alors que sur les modèles de fréquences un $mtry$ faible (moins de $5$) fera l'affaire. Nous allons quand même essayer plusieurs statégies pour améliorer nos prédictions, notamment comment exploiter la variable $Exposure$?.
On prendra $mtry=3$ pour la séléction des meilleurs modèles.

### B. Réflexion sur la variable $Exposure$

Nous avons essayé trois modélisations différentes sur un petit set de donnée :

* En utilisant la variable $Exposure$ comme variable explicative

* En utilisant la variable $Exposure$ comme offset

* En divisant $ClaimNb$ par $Exposure$

Les résultats sont les suivants : 

```{r comparaison foret exposure, echo = FALSE, fig.height=7, fig.width=10}

load("data/rf_freq_mtry_3_ntree_500_Exposure_as_weights.rda")
load("data/rf_freq_mtry_3_ntree_500_Exposure_devided.rda")
load("data/rf_freq_mtry_3_ntree_500_Exposure_exp.rda")

predict(rf_freq_mtry_3_ntree_500_Exposure_as_weights, test, weights = test$Exposure) -> pred_rf_freq_mtry_3_ntree_500_Exposure_as_weights
predict(rf_freq_mtry_3_ntree_500_Exposure_devided, test) * test$Exposure -> pred_rf_freq_mtry_3_ntree_500_Exposure_devided
predict(rf_freq_mtry_3_ntree_500_Exposure_exp, test) -> pred_rf_freq_mtry_3_ntree_500_Exposure_exp

RMSE(pred = pred_rf_freq_mtry_3_ntree_500_Exposure_as_weights, test$ClaimNb) -> RMSE_1
RMSE(pred = pred_rf_freq_mtry_3_ntree_500_Exposure_devided, test$ClaimNb) -> RMSE_2
RMSE(pred = pred_rf_freq_mtry_3_ntree_500_Exposure_exp, test$ClaimNb) -> RMSE_3

resultat <- data.frame(matrix("", 3, 2))
colnames(resultat) <- c("Gestion de l'exposition", "RMSE sur donnée de test")
resultat[,1] <- c("Exposition en offset", "Division par Exposition", "Exposition en explicative")
resultat[,2] <- c(RMSE_1, RMSE_2, RMSE_3)

print(resultat)

```

Les résultats sont légèrement meilleurs quand l'exposition est en variable explicative, ce qui est douteux. Probablement du fait que le dataset était tout petit. On va pousser la taille de l'échantillon et finir par convenir à un modèle final.

### C. Modèles finaux

Comparons maintenant les résultats obtenus avec les deux modèles retenus pour la fréquence

```{r selection meilleur rf freq, echo = FALSE}

load("data/rf_freq_final_expo_as_exp.rda")
load("data/rf_freq_final_expo_as_weights.rda")

best_rf_model_freq <- data.frame(matrix(0, 2, 4), row.names = c("Exposition comme poids", "Exposition comme variable explicative"))

colnames(best_rf_model_freq) <- c("Proportion de donnée","RMSE entrainement", "RMSE test", "Biais test")

best_rf_model_freq[,1] <- c("15%", "15%")
best_rf_model_freq[,2] <- c(rf_freq_final_expo_as_weights$results$RMSE, rf_freq_final_expo_as_exp$results$RMSE)
best_rf_model_freq[,3] <- c(RMSE(pred = predict(rf_freq_final_expo_as_weights, test, weights = test$Exposure), obs = test$ClaimNb), RMSE(pred = predict(rf_freq_final_expo_as_exp, test), obs = test$ClaimNb))
best_rf_model_freq[,4] <- c(mean(predict(rf_freq_final_expo_as_weights,test, weights = test$ClaimNb)) - mean(test$ClaimNb), mean(predict(rf_freq_final_expo_as_exp, test)) - mean(test$ClaimNb))

print(best_rf_model_freq)

```
Les deux modèles se vaux. En effet, l'un est meilleur en $RMSE$ mais est moins bien en Biais. On va préferer le modèle avec l'exposition comme offset car le biais est négatif; ainsi, on sur-estime la fréquence d'apparition des sinistres.

# IV. Gradiant boosting machine

```{r data, visible = FALSE, echo = FALSE}
library(caret)
library(xgboost)
library(dplyr)
load("./data/train&valid&test.RData")
train.cout <- list(data = train[which(train$ClaimNb>0),]
                   , label = train$MeanClaimAmount[which(train$ClaimNb>0)]
)

valid.cout <- list(data = valid[which(valid$ClaimNb>0),]
                   , label = valid$MeanClaimAmount[which(valid$ClaimNb>0)]
)


train.cout.xgb <<- xgb.DMatrix(data = cbind(predict(dummyVars(data=train.cout$data,formula = "~Area"), newdata = train.cout$data)
                                            , predict(dummyVars(data=train.cout$data,formula = "~VehPower"), newdata = train.cout$data)
                                            , VehAge = train.cout$data$VehAge
                                            , DrivAge = train.cout$data$DrivAge
                                            , BonusMalus = train.cout$data$BonusMalus
                                            , predict(dummyVars(data=train.cout$data,formula = "~VehBrand"), newdata = train.cout$data)
                                            , predict(dummyVars(data=train.cout$data,formula = "~VehGas"), newdata = train.cout$data)
                                            , Density = train.cout$data$Density
                                            , predict(dummyVars(data=train.cout$data,formula = "~Region"), newdata = train.cout$data)
)
, label = train.cout$label)

valid.cout.xgb <<- xgb.DMatrix(data = cbind(predict(dummyVars(data=valid.cout$data,formula = "~Area"), newdata = valid.cout$data)
                                            , predict(dummyVars(data=valid.cout$data,formula = "~VehPower"), newdata = valid.cout$data)
                                            , VehAge = valid.cout$data$VehAge
                                            , DrivAge = valid.cout$data$DrivAge
                                            , BonusMalus = valid.cout$data$BonusMalus
                                            , predict(dummyVars(data=valid.cout$data,formula = "~VehBrand"), newdata = valid.cout$data)
                                            , predict(dummyVars(data=valid.cout$data,formula = "~VehGas"), newdata = valid.cout$data)
                                            , Density = valid.cout$data$Density
                                            , predict(dummyVars(data=valid.cout$data,formula = "~Region"), newdata = valid.cout$data)
)
, label = valid.cout$label)






train.freq <- list(data = train[which(train$AnnualClaimNb<=10),]
                   , label = train$AnnualClaimNb[which(train$AnnualClaimNb<=10)]
)

valid.freq <- list(data = valid[which(valid$AnnualClaimNb<=10),]
                   , label = valid$AnnualClaimNb[which(valid$AnnualClaimNb<=10)]
)



train.freq.xgb = xgb.DMatrix(data = as.matrix(cbind(predict(dummyVars(data=train.freq$data,formula = "~Area"), newdata = train.freq$data)
                                                    , predict(dummyVars(data=train.freq$data,formula = "~VehPower"), newdata = train.freq$data)
                                                    , VehAge = train.freq$data$VehAge
                                                    , DrivAge = train.freq$data$DrivAge
                                                    , BonusMalus = train.freq$data$BonusMalus
                                                    , predict(dummyVars(data=train.freq$data,formula = "~VehBrand"), newdata = train.freq$data)
                                                    , predict(dummyVars(data=train.freq$data,formula = "~VehGas"), newdata = train.freq$data)
                                                    , Density = train.freq$data$Density
                                                    , predict(dummyVars(data=train.freq$data,formula = "~Region"), newdata = train.freq$data)))
                             , label = train.freq$label)


valid.freq.xgb = xgb.DMatrix(data = as.matrix(cbind(predict(dummyVars(data=valid.freq$data,formula = "~Area"), newdata = valid.freq$data)
                                                    , predict(dummyVars(data=valid.freq$data,formula = "~VehPower"), newdata = valid.freq$data)
                                                    , VehAge = valid.freq$data$VehAge
                                                    , DrivAge = valid.freq$data$DrivAge
                                                    , BonusMalus = valid.freq$data$BonusMalus
                                                    , predict(dummyVars(data=valid.freq$data,formula = "~VehBrand"), newdata = valid.freq$data)
                                                    , predict(dummyVars(data=valid.freq$data,formula = "~VehGas"), newdata = valid.freq$data)
                                                    , Density = valid.freq$data$Density
                                                    , predict(dummyVars(data=valid.freq$data,formula = "~Region"), newdata = valid.freq$data)))
                             , label = valid.freq$label)


load("./data/trainandtest.rda")
test <- cbind(test, AnnualClaimNb = test$ClaimNb/test$Exposure)
test.cout <- list(data = test[which(test$ClaimNb>0),]
                  , label = test$AnnualClaimNb[which(test$ClaimNb>0)])


test.cout.xgb <- xgb.DMatrix(data = cbind(predict(dummyVars(data=test.cout$data,formula = "~Area"), newdata = test.cout$data)
                                           , predict(dummyVars(data=test.cout$data,formula = "~VehPower"), newdata = test.cout$data)
                                           , VehAge = test.cout$data$VehAge
                                           , DrivAge = test.cout$data$DrivAge
                                           , BonusMalus = test.cout$data$BonusMalus
                                           , predict(dummyVars(data=test.cout$data,formula = "~VehBrand"), newdata = test.cout$data)
                                           , predict(dummyVars(data=test.cout$data,formula = "~VehGas"), newdata = test.cout$data)
                                           , Density = test.cout$data$Density
                                           , predict(dummyVars(data=test.cout$data,formula = "~Region"), newdata = test.cout$data)
)
, label = test.cout$label)

test.freq <- list(data = test[which(test$AnnualClaimNb<=10),]
                  , label = test$AnnualClaimNb[which(test$AnnualClaimNb<=10)]
)


test.freq.xgb = xgb.DMatrix(data = as.matrix(cbind(predict(dummyVars(data=test.freq$data,formula = "~Area"), newdata = test.freq$data)
                                                   , predict(dummyVars(data=test.freq$data,formula = "~VehPower"), newdata = test.freq$data)
                                                   , VehAge = test.freq$data$VehAge
                                                   , DrivAge = test.freq$data$DrivAge
                                                   , BonusMalus = test.freq$data$BonusMalus
                                                   , predict(dummyVars(data=test.freq$data,formula = "~VehBrand"), newdata = test.freq$data)
                                                   , predict(dummyVars(data=test.freq$data,formula = "~VehGas"), newdata = test.freq$data)
                                                   , Density = test.freq$data$Density
                                                   , predict(dummyVars(data=test.freq$data,formula = "~Region"), newdata = test.freq$data)))
                            , label = test.freq$label)


mod.cout.xgb <- xgb.load("./xgboost/mode.cout.xgb.xgboost")
mod.freq.xgb <- xgb.load("./xgboost/mod.freq.xgb.xgboost")


```


## A. Préparation des données
Dans cette partie nous décrivons succintement les résultats obtenus par une modélisation GBM avec arbres de régression. Nous procédons à une décomposition fréquence coût moyen. Dans toute cette partie, nous utiliserons le package "xgboost", qui implémente de façon plus efficiente le GBM que le package "gbm".

Sans entrer dans les détails techniques, nous listons ici les différents problèmes rencontrés dans la mise en forme des données pour le package xgboost.

1. La fonction de fit ne prend en entrée que des variables numériques (c'est classique en machine learning): nous utilisons le package "caret" pour créer des flags de nos variables catégorielles.
2. Pour utiliser la fonction la plus complète du package pour l'entrainement (xgb.train), nous devons passer les données en entrée au format xgb.DMatrix
3. Pour la modélisation de la fréquence, où la base utilisée est déjà très volumineuse, créer des flags de variables catégorielles avec relativement beaucoup de modalités pose des problèmes de stockage (en effet, nos pc ne sont pas des machines de guerre):  
    3.1. Une des fonctions, xgboost, permet de passer en paramètre des sparse Matrix, mais la fonction est moins complète au niveau du contrôle des paramètres de tuning.
    3.2. Nous créons sur une machine puissante les flags, et nos enregistrons les données au format xgb.DMatrix en dur. 
                                                                       
## B. Modèle de coût
                                                                       
On optimise les paramètres de tuning avec une grid search à l'aide du package caret: on fixe un petit nombre d'arbres (10) pour entrainemer les modèles et on cherche les paramètre optimaux à l'aide d'une cross validation. On utilise le RMSE comme critère de choix de modèle.

On identifie des paramètres important pour éviter l'overfitting: min_child_weight, subsample, col_sample.

Le RMSE semble être plus sensibles aux paramètres max_depth (la profondeur maximale des arbres), nround (le nombre d'arbre).

Le temps de calcul est très sensible aux paramètres max_depth et nrounds. En outre, le temps de calcul est très sensibles au volume des données passées en entrée.

Nos paramètres optimaux sont:

Nous cherchons maintenantle nombre otpimal d'arbres. Nous obtenons 386 arbres.
  
Voici une représentation graphique du RMSE en fonction du nombre d'abres:
  
  
  
Nous pouvons également nous intéresser à l'influence respective de chaque variable explicative:

``` {r imp.cout, echo = FALSE}

imp <- xgb.importance(c("VehGas", "Density", "Region", "VehBrand", "VehPower", "DrivAge", "Area", "BonusMalus", "VehAge")
, model = mod.cout.xgb)

barplot(imp$Gain, names.arg = imp$Feature, col = "blue", main = "Importance des variables explicatives dans le modèle", ylab = "%")

```

Arrive en tête la variable BonusMalus suive de très près par la variable VehAge.


```{r predict.cout.biais, echo = FALSE} 


pred.cout <- predict(mod.cout.xgb, test.cout.xgb)
(mean(pred.cout) - mean(test.cout$label)) -> xgb.test.cout.biais



```
Le biais que présente notre modèle d'environ $xgb.test.cout.biais$

Cela signifie que l'on surestime en moyenne le montant des sinistres.



```{r predict.cout.rmse, echo = FALSE} 

((pred.cout - test.cout$label)^2 %>% mean() %>% sqrt()) -> xgb.test.cout.RMSE

```

Le RMSE sur la base de test est $xgb.test.cout.RMSE$.

Le test de notre modèle sur la base de test révèle que notre modèle possède un biais de plusieurs dizaines, à la baisse. Cela signifie que nous sous-estimons les montants des sinistres attritionnels.


## C. Modèle de fréquence

Nous devons prendre en compte l'exposition. Nous définissons la variable à expliquer comme le nombre annuel de sinistres. Nous divisons donc le nombre de sinistres par l'exposition pour chaque police et nous pondérons également les observations par l'exposition.

Nous ne cherchons pas en profondeur les paramètres optimaux. Nous retenons les paramètres de tuning suivants à la main en parcourant une courte grille de paramètres. Nous ne parvenons pas à utiliser le package caret pour cette tâche. Nous avons donc créé une petite fonction pour palier à ça. Nous calcul simplement le RMSE sur la base de validation pour choisir les paramètres.


On évalue ensuite l'influence des différentes variables explicatives.
``` {r imp.freq, echo = FALSE}

imp <- xgb.importance(c("VehGas", "Density", "Region", "VehBrand", "VehPower", "DrivAge", "Area", "BonusMalus", "VehAge")
, model = mod.freq.xgb)

barplot(imp$Gain, names.arg = imp$Feature, col = "blue", main = "Importance des variables explicatives dans le modèle", ylab = "%", las = 2)

```

Les variables arrivant en tête sont BonusMalus et VehAge.

Notre modèle présente également un biais de:
```{r predict.freq.biais, echo = FALSE} 


pred.freq <- predict(mod.freq.xgb, test.freq.xgb)
mean(pred.freq) - mean(test.freq$label)

(pred.freq - test.freq$label)^2 %>% mean() %>% sqrt() -> xgb.test.freq.RMSE 



```
Cela signifie que l'on sous-estime très légèrement en moyenne notre fréquence de sinistres.

Le RMSE sur la base de test est de:
```{r predict.freq.RMSE, echo = FALSE} 

(pred.freq - test.freq$label)^2 %>% mean() %>% sqrt() 

```

Les tests sur la base de test révèlent que notre modèle est également biaisé. 



# V. Réseaux de neuronnes

    A. Présentation du procédé

Le réseau de neurones artificiels a été inspiré du mécanisme du système nerveux humain, à travers l’histoire on a cherché à imiter son fonctionnement en commençant par le neurone formel apparu en 1943 avec une modélisation mathématique  du neurone biologique, puis par les premières règles d’apprentissage de Hebb en 1949 et de l’algorithme d’apprentissage de rétropropagation en 1986 par Rumelhart.

Cet algorithme  est utilisé dans le  cadre des problématiques de classification, de reconnaissance de formes, d’association, d’extractions  de caractéristiques, d’identification…

En assurance, l’avantage de cet algorithme est de pou voir capter les dépendances non linéaires entre les variables explicatives pour pouvoir affiner le tarif et appréhender le risque.

Pour les parties qui suivent, le  paramètre le plus  important  qui serait à déterminer est  représenté par le nombre de neurones sur la couche cachée parallèlement aux conditions d’apprentissage que nous citerons par la suite:

Pour le calibrage des deux modèles Fréquence et Coût, nous avons utilisé plusieurs fonctions  intégrées sous R:

- Tune.nnet du package
- Nnet du package

# VI. Conclusion 

# VII. Annexes et références

## 1. Annexes

## 2. Références
