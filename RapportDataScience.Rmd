---
title: "Rapport Data Science"
author: "Mehdi KHAIROUN, William LAURENT, Pierre MARJOLLET, Afaf TABAMMOUT"
date: "9 avril 2018"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

![](images/logoisfa.jpg) 

***

# I. Matériel et pré-processing

## 1. Jeu de données

Les données étudiées sont deux bases issues de la librairie CASdatasets (CAS pour Computational Actuarial Sciences). **freMTPL2freq** et **freMTPL2sev** sont deux bases autos.

Brièvement **freMTPL2freq** est une base de fréquence, nous donnant des caractéristiques pour des polices d'assurances données et leurs nombre de sinitres respectifs.**Cette base dispose de 12 variables et 678 013 individus**, comprenant :

* **IDpol** [Clé primaire : Entier de 1 à 6 114 330] : l'identifiant de la police d'assurance
* **ClaimNb** [Entier de 0 à 16] : le nombre de sinistres de la police
* **Exposure** [Réel de 0 à 1] : l'exposition - la durée de vie de la police. De 0 pour une personne non assuré à 1 pour une personne assurée toute l'année
* **Area** [Modalité de A à F] : indication sur la zone d'apparition du sinistre
* **VehPower** [Entier de 5 à 15] : puissance du véhicule
* **VehAge** [Entier de 0 a 100] : age du véhicule
* **DrivAge** [Entier de 18 à 100] : age du conducteur
* **BonusMalus** [Entier de 50 à 230] : bonus/malus du conducteur. Plus le score est haut, moins l'assuré est sanctionné par son passé
* **VehBrabd** [Modalité de B1 à B12] : marque du véhicule
* **VehGas** [Booléent : 0 ou 1] : véhicule diéserl ou non
* **Density** [Entier de 1 à 1658] : densité de population de la police
* **Region** [24 modalité de R11 à R94] : code région de la police

Enfin, **freMTPLsev** est une base sévèrité, avec simplement un identifiant et une valeur de sinistre. **Cette base comprends 2 variables et 26 639 individus**:

* **IDpol** [Entier de 139 à 6113971] : l'identifiant de la police d'assurance
* **ClaimAmount** [Réel de 1 à 4 075 401] : coût du sinistre

## 2. Pré-processing

Nous avons calculé la moyenne des coûts des sinistres par polices sur la base **freMTPLsev**. Le but étant de savoir quelle est la moyenne des sinistres d'une police.

Nous avons ensuite fusionné les bases **freMTPLsev** et **freMTPLfreq** par **IDpol** maintenant unique sur les deux bases.

Il vient ensuite une base sur laquelle nous retirons les sinistres graves (ceux supérieures à $20 000$). Ce seuil a été déterminé rapidement par l'étude d'un mean-excess plot. Ces quelques sinistres biaisais fortement nos modélisations et seront répartie comme une sur-prime sur l'ensemble de nos assurés.

Nous basons nos modélisations sur seulement $70%$ de nos données pour se laisser une bonne marge de test. Ensuite, pour les procédures plus lourde (typiquement le gradian boosting et les réseaux de neuronnes) les derniers modèles retenus ont été entraîner sur un sous jeux de cette base d'entraînement.

Pour les modélisations en sévérité, nous avons biensûre enlevé les polices ne présentant aucun sinistre.

## 3. R et ses extentions

Dans ce projet nous utilisons le logiciel opensource R et nous utilisons son extention RMarkdown pour la rédaction de ce rapport.

Pour pouvoir mener à bien ce projet, nous avons munit R des librairies suivantes :

* **caret** : **C**lassification **a**nd **re**gression **t**raining, utilisé ici pour utiliser nos algorithmes aisément sur des grilles de paramètre de tuning. Propose également une parrallélisation de ses algorithmes. **caret** dispose de tous les algorithmes nécessaire au bon déroulement de ce projet et pourrait nous suffire ici.
* **doParallel** : s'accouple à **caret** pour accéder à la parrallélisation des algorithmes et ainsi économiser quelques précieuses heures de run.
* **rpart** : permets de dresser des arbres (ici des arbres de régressions).
* **rpart.plot** : utilisé ici pour afficher des graph d'arbres de régressions.
* **magrittr** : donner au code plus de lisibilité.
* **etc**

# II. Arbres de régression

# III. Forêts aléatoires

## 1. Présentation du procédé

Avant de parler de random forests (forêts aléatoires) parlons de Bagging. Le bagging de Bootstrap Aggregating est simplement un bootstrap sur des modélisations données. C'est une méthode d'aggrégation de modèles. Nous avons fait poussé des arbres dans la partie précédente; nous allons ici en faire poussé beaucoup et les aggrégés pour fabriquer des forêts!

La méthode de random forest que nous allons exploiter ici est celle qui consiste à faire un Bagging sur des arbres de régressions maximaux. On va faire pousser une forêt de $ntree$ arbres et chacun de ces arbres aura un choix aléatoire de $mtry$ variables a chaque noeud. Le seul hyperparamètre qui nous intéressera par la suite est $mtry$. $ntree$ lui est un paramètre sur la convergence des données, il va de sois qu'avec un grand $ntree$ nous convergerons que mieux. $mtry$ lui :

* **Pour des petites valeurs** : les noeuds disposerons que de peu de variable pour se former, l'arbre peu être unitairement pauvre en puissance prédictive mais la correlation entre les arbres est minimisée.
* **Pour des grandes valeurs** : les noeuds disposerons de beaucoup de variable pour se former, l'arbre peu être riche en puissance prédictive (comme un CART maximal) mais la correlation entre les arbres sera forte
* Ainsi,un $mtry$ sera déterminé par grille de paramêtres.

## 2. Modèlisation du coût moyen

Nous avons fais tourner une 

```{r plot, echo = FALSE, fig.height=7, fig.width=10}

load("data/10percentsRF.rdata")

plot(
  rf10
  , main = "Evolution de la RMSE sur l'échantillon d'entrainement en fonction de mtry\nModèle sur la sévérité"
  , xlab = "mtry"
  , ylab = "rmse"
  , sub = "25 échantillons bootstrap sur 10% de la donnée, 500 arbres par forêt"
  )

```
Sur ce graph on trouve que la région des bons $mtry$ se trouve entre $10$ et $20$. Dans la pratique, pour un problème de régression, randomForest conseillera de prendre la racine du nombre de variable. Chez nous, il en faut un peu plus.



Même si chaque arbre sur-interprête, mais les forêts étonnament ne le font pas. 

mtry ; si nous le limitons, c'est pour minimiser la correlation entre les arbres de la forêts

# IV. Gradiant boosting machine
     A. Préparation des données
Dans cette partie nous décrivons succintement les résultats obtenus par une modélisation GBM avec arbres de régression. Nous procédons à une décomposition fréquence coût moyen. Dans toute cette partie, nous utiliserons le package "xgboost", qui implémente de façon plus efficiente le GBM que le package "gbm".

Sans entrer dans les détails techniques, nous listons ici les différents problèmes rencontrés dans la mise en forme des données pour le package xgboost.
 
1. La fonction de fit ne prend en entrée que des variables numériques (c'est classique en machine learning): nous utilisons le package "caret" pour créer des flags de nos variables catégorielles.
2. Pour utiliser la fonction la plus complète du package pour l'entrainement (xgb.train), nous devons passer les données en entrée au format xgb.DMatrix
2. Pour la modélisation de la fréquence, où la base utilisée est déjà très volumineuse, créer des flags de variables catégorielles avec relativement beaucoup de modalités pose des problèmes de stockage (en effet, nos pc ne sont pas des machines de guerre):
    1.1. Une des fonctions, xgboost, permet de passer en paramètre des sparse Matrix, mais la fonction est moins complète au niveau du contrôle des paramètres de tuning.
    1.2. Nous créons sur une machine puissante les flags, et nos enregistrons les données au format xgb.DMatrix en dur. 

     B. Modèle de coût

On optimise les paramètres de tuning avec une grid search à l'aide du package caret: on fixe un petit nombre d'arbres (10) pour entrainemer les modèles et on cherche les paramètre optimaux à l'aide d'une cross validation. On utilise le RMSE comme critère de choix de modèle.
On identifie des paramètres important pour éviter l'overfitting: min_child_weight, subsample, col_sample.
Le RMSE semble être plus sensibles aux paramètres max_depth (la profondeur maximale des arbres), nround (le nombre d'arbre).
Le temps de calcul est très sensible aux paramètres max_depth et nrounds. En outre, le temps de calcul est très sensibles au volume des données passées en entrée.

Nos paramètres optimaux sont:


Voici quelques graphiques illustrant ces sensibilités à partir de nos paramètres de tuning optimaux


Nous cherchons maintenantle nombre otpimal d'arbres. Nous obtenons:
Et voici une représentation graphique du RMSE en fonction du nombre d'abres:



Nous pouvons également nous intéresser à l'influence respective de chaque variable explicative:


Le test de notre modèle sur la base de test révèle que notre modèle possède un biais de plusieur dizaines, à la baisse. Cela signifie que nous sous-estimons les montants des sinistres attritionnels.


  B. Modèle de fréquence

Nous devons prendre en compte l'exposition. Nous définissons la variable à expliquer comme le nombre annuel de sinistres. Nous divisons donc le nombre de sinistres par l'exposition pour chaque police, nous multiplierons la prediction par l'exposition. Nous pondérons également les observations par l'exposition.

Nous ne cherchons pas en profondeur les paramètres optimaux. Nous retenons les paramètres de tuning suivants:

Les tests sur la base de test révèlent que notre modèle est également biaisé. 



# V. Réseaux de neuronnes

    A. Présentation du procédé

Le réseau de neurones artificiels a été inspiré du mécanisme du système nerveux humain, à travers l’histoire on a cherché à imiter son fonctionnement en commençant par le neurone formel apparu en 1943 avec une modélisation mathématique  du neurone biologique, puis par les premières règles d’apprentissage de Hebb en 1949 et de l’algorithme d’apprentissage de rétropropagation en 1986 par Rumelhart.

Cet algorithme  est utilisé dans le  cadre des problématiques de classification, de reconnaissance de formes, d’association, d’extractions  de caractéristiques, d’identification…

En assurance, l’avantage de cet algorithme est de pou voir capter les dépendances non linéaires entre les variables explicatives pour pouvoir affiner le tarif et appréhender le risque.

Pour les parties qui suivent, le  paramètre le plus  important  qui serait à déterminer est  représenté par le nombre de neurones sur la couche cachée parallèlement aux conditions d’apprentissage que nous citerons par la suite:

Pour le calibrage des deux modèles Fréquence et Coût, nous avons utilisé plusieurs fonctions  intégrées sous R:

- Tune.nnet du package
- Nnet du package

# VI. Conclusion 

# VII. Annexes et références

## 1. Annexes

## 2. Références