#si ksi > -1/2 , l'EMV est régulier (les lois limites sont usuelles)
#si -1 < ksi < -1/2, l'estimateur est super efficace
#si ksi < -1, l'estimateur du point extrêma est donné par la plus grande des observations
#ici ksi = O
# fit1 %>% plot()
fit1 %>% rlevel.gev(.,100)
##############################################################################
#############___________    Exercice 1
##############################################################################
library(evir)
library(dplyr)
N <- 1000
size <- 100
exp1 <- matrix(rexp(N*size),size, N)
M <- apply(exp1, 1, max)
#?gev
fit1 <- gev(data = M)
#si ksi > -1/2 , l'EMV est régulier (les lois limites sont usuelles)
#si -1 < ksi < -1/2, l'estimateur est super efficace
#si ksi < -1, l'estimateur du point extrêma est donné par la plus grande des observations
#ici ksi = O
# fit1 %>% plot()
fit1 %>% rlevel.gev(.,100)
##############################################################################
#############___________    Exercice 1
##############################################################################
library(evir)
library(dplyr)
N <- 1000
size <- 100
exp1 <- matrix(rexp(N*size),size, N)
M <- apply(exp1, 1, max)
#?gev
fit1 <- gev(data = M)
#si ksi > -1/2 , l'EMV est régulier (les lois limites sont usuelles)
#si -1 < ksi < -1/2, l'estimateur est super efficace
#si ksi < -1, l'estimateur du point extrêma est donné par la plus grande des observations
#ici ksi = O
# fit1 %>% plot()
fit1 %>% rlevel.gev(.,100)
##############################################################################
#############___________    Exercice 1
##############################################################################
library(evir)
library(dplyr)
N <- 1000
size <- 100
exp1 <- matrix(rexp(N*size),size, N)
M <- apply(exp1, 1, max)
#?gev
fit1 <- gev(data = M)
#si ksi > -1/2 , l'EMV est régulier (les lois limites sont usuelles)
#si -1 < ksi < -1/2, l'estimateur est super efficace
#si ksi < -1, l'estimateur du point extrêma est donné par la plus grande des observations
#ici ksi = O
# fit1 %>% plot()
fit1 %>% rlevel.gev(.,100)
fit1
data <- data("nidd.annual")
data
View(data)
data("nidd.annual")
nidd.annual
nidd.annual %>% plot()
fit2 <- gev(nidd.annual)
fit2
fit2 %>% rlevel.gev(., 100)
##############################################################################
#############___________    Exercice 2
##############################################################################
data("danish")
fit2 %>% plot()
meplot(danish)
install.packages(tea)
install.packages("tea")
tea::ggplot(danish)
u <- danish[298]
u
danish %>% mean()
fit3 <- gpd(data = danish)
fit3 <- gpd(data = danish, threshold = u)
fit3
fit3 <- gpd(data = danish, threshold = min(danish))
fit4 <- gpd(data = danish, threshold = 5)
fit4
xgb_grid_1 = expand.grid(nrounds = 50
, max_depth = c(3,6,9)
, eta = c(0.01, 0.001, 0.0001)
, gamma = c(0.3,0.7,1)
, colsample_bytree = c(0.8,1)
, min_child_weight = c(0.5,0.7,1)
, subsample = 0.8
)
xgb_grid_1
#here we do one better then a validation set, we use cross validation to
#expand the amount of info we have!
xgb_trcontrol_1 = trainControl(method = "cv",
number = 7,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",
allowParallel = TRUE)
xgb_train_1 = train(x = as.matrix(lbase.cout.oh$train[,-c(1,2,3,length(lbase.cout.oh$full[1,]))]),
y = lbase.cout.oh$train[,'MeanClaimAmount'],
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbTree")
setwd("~/ISFA/3A/Data Science/ProjetDataScience/DataScience")
#setwd("~/DataScience-master")
library(magrittr)
library(rpart)
library(rpart.plot)
#setwd("~/ISFA/3A/Data Science/ProjetDataScience/DataScience/data")
#Initialisation des donnees ----
# load("data/freMTPL2freq.rda")
# load("data/freMTPL2sev.rda")
data("freMTPL2freq")
data("freMTPL2sev")
# On récupère les deux datasets :
data(freMTPL2freq)
data(freMTPL2sev)
frequence <- freMTPL2freq
severite <- freMTPL2sev
rm(freMTPL2freq, freMTPL2sev)
#Mise en forme des donnéees ----
#Enlèvons Density?
#frequence <-frequence[, ! colnames(frequence) %in% "Density"]
#On formate les données de la base de frequence
frequence$IDpol <- as.factor(frequence$IDpol)
frequence$ClaimNb <- frequence$ClaimNb %>% unname() %>% as.numeric()
frequence$VehPower <- as.integer(frequence$VehPower)
frequence$Exposure <- as.double(frequence$Exposure)
frequence$Area <- as.factor(frequence$Area)
frequence$VehAge <- as.integer(frequence$VehAge)
frequence$DrivAge <- as.integer(frequence$DrivAge)
frequence$BonusMalus <- as.integer(frequence$BonusMalus)
frequence$VehBrand <- as.factor(frequence$VehBrand)
frequence$VehGas <- as.factor(frequence$VehGas)
frequence$Region <- as.factor(frequence$Region)
#On formate la base de cout
severite$IDpol <- as.factor(severite$IDpol)
severite$ClaimAmount <- as.numeric(severite$ClaimAmount)
#On prends que les exposures inférieures à 1 ?
frequence <- frequence[frequence$Exposure <= 1,]
#######################################################################################################
######################_________________________   Gradient boosting
#######################################################################################################
library(caret)
library(xgboost)
library(fExtremes)
library(gbm)
library(onehot)
###########################
#########_____    Fonctions utiles
###########################
# #fonction pour le calcul du taux d’erreur
# err_rate <- function(D,prediction){
#   #matrice de confusion
#   mc <- table(D$chiffre,prediction)
#   #taux d’erreur
#   #1- somme(individus classés correctement) / somme totale individus
#   err <- 1 - sum(diag(mc))/sum(mc)
#   print(paste("Error rate :",round(100*err,2),"%"))
# }
###########################
#########_____    Mise en forme, Ecrètement et tarification des sinistres graves
###########################
mePlot(severite$ClaimAmount)
.seuil.grv <- 20000
nrow(severite[which(severite$ClaimAmount>.seuil.grv),])/nrow(severite)*100
att <- data.frame(severite[which(severite$ClaimAmount<=.seuil.grv & severite$ClaimAmount>100),]
,rep(1,nrow(severite[which(severite$ClaimAmount<=.seuil.grv & severite$ClaimAmount>100),]))
)
names(att) <- c("IDpol","ClaimAmount","AttClaimNb")
att.mean <- aggregate(ClaimAmount ~ IDpol, data = att, mean)
#On renomme
names(att.mean) <- c("IDpol", "MeanClaimAmount")
base <- merge(x = frequence, y = att.mean, by = "IDpol", all.x = T)
base$MeanClaimAmount <- replace(base$MeanClaimAmount, is.na(base$MeanClaimAmount), 0)
head(base,5)
summary(base)
base <- base[-which(base$ClaimNb >0 & base$MeanClaimAmount ==0),]
#base <- base[-which(base$ClaimNb>5),]
grave <- data.frame(severite[which(severite$ClaimAmount>.seuil.grv),],rep(1,nrow(severite[which(severite$ClaimAmount>.seuil.grv),])))
names(grave) <- c("IDpol","GrvClaimAmount","GrvClaimNb")
surprime.grv <- sum(grave$GrvClaimAmount)/sum(base$Exposure)
#La surprime est de ...
surprime.grv
###########################################################
##############################_____ Modèle de cout
###########################################################
#On ne garde que les polices sinistrées
base.cout <- base[which(base$ClaimNb>0),]
################################################
###################_____    Séparation des données en 3 jeux: train, validation et test
################################################
#Les proportions que l'on veut pour nos différentes bases
proportion.train <- 0.75
porportion.valid <- 0.125
proportion.test <- 0.125
#On stocke toute les bases dans une liste de base, c'est plus simple à utiliser
#Les indices pour la base de train
sample.train <- sample.int(n = nrow(base.cout)
, size = floor(proportion.train*nrow(base.cout))
, replace = F)
#Une base temporaire qui les reste de la base initiale après la division pour la de train
base.temp <- base.cout[-sample.train,]
sample.test <- base.temp %>% nrow() %>% sample.int(n=.
, size = floor(porportion.valid/proportion.train*.)
, replace = F)
#On crée les bases et la liste
lbase.cout <- list(full = base.cout
, train = base.cout[sample.train,]
, test = base.temp[sample.test,]
, valid = base.temp[-sample.test,])
head(lbase.cout$train,10)
head(lbase.cout$valid,10)
head(lbase.cout$test,10)
######## Flags (pour xgboost)
base.cout.oh <- predict(onehot(base.cout, stringsAsFactors = T, addNA = FALSE, max_levels = 100)
, base.cout)
base.temp <- base.cout.oh[-sample.train,]
sample.test <- base.temp[,1] %>% length() %>% sample.int(n=.
, size = floor(porportion.valid/proportion.train*.)
, replace = F)
#On crée les bases et la liste
lbase.cout.oh <- list(full = base.cout.oh
, train = base.cout.oh[sample.train,]
, test = base.temp[sample.test,]
, valid = base.temp[-sample.test,])
head(lbase.cout.oh$train,10)
head(lbase.cout.oh$valid,10)
head(lbase.cout.oh$test,10)
# #On crée une liste des bases au format xgbMatrix
lbase.cout.xgb <- list(train = xgb.DMatrix(data = as.matrix(lbase.cout.oh$train[,-c(1,2,3,length(lbase.cout.oh$full[1,]))])
, label = lbase.cout.oh$train[,'MeanClaimAmount'])
,test = xgb.DMatrix(data = as.matrix(lbase.cout.oh$test[,-c(1,2,3,length(lbase.cout.oh$full[1,]))])
, label = lbase.cout.oh$test[,'MeanClaimAmount'])
,valid = xgb.DMatrix(data = as.matrix(lbase.cout.oh$valid[,-c(1,2,3,length(lbase.cout.oh$full[1,]))])
, label = lbase.cout.oh$valid[,'MeanClaimAmount'])
)
################################################
###################_____    Modélisation
################################################
xgb_grid_1 = expand.grid(nrounds = 50
, max_depth = c(3,6,9)
, eta = c(0.01, 0.001, 0.0001)
, gamma = c(0.3,0.7,1)
, colsample_bytree = c(0.8,1)
, min_child_weight = c(0.5,0.7,1)
, subsample = 0.8
)
xgb_grid_1
#here we do one better then a validation set, we use cross validation to
#expand the amount of info we have!
xgb_trcontrol_1 = trainControl(method = "cv",
number = 7,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",
allowParallel = TRUE)
xgb_train_1 = train(x = as.matrix(lbase.cout.oh$train[,-c(1,2,3,length(lbase.cout.oh$full[1,]))]),
y = lbase.cout.oh$train[,'MeanClaimAmount'],
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbTree")
?seq
xgb_grid_2 = expand.grid(nrounds = seq(from = 1, to = 1001, by = 5)
, max_depth = xgb_train_1$bestTune$max_depth
, eta = xgb_train_1$bestTune$eta
, gamma = xgb_train_1$bestTune$gamma
, colsample_bytree = xgb_train_1$bestTune$colsample_bytree
, min_child_weight = xgb_train_1$bestTune$min_child_weight
, subsample = xgb_train_1$bestTune$subsample
)
xgb_train_2 = train(x = as.matrix(lbase.cout.oh$train[,-c(1,2,3,length(lbase.cout.oh$full[1,]))]),
y = lbase.cout.oh$train[,'MeanClaimAmount'],
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_2,
method = "xgbTree")
xgb_trcontrol_1 = trainControl(method = "cv",
number = 1,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",
allowParallel = TRUE)
xgb_train_2 = train(x = as.matrix(lbase.cout.oh$train[,-c(1,2,3,length(lbase.cout.oh$full[1,]))]),
y = lbase.cout.oh$train[,'MeanClaimAmount'],
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_2,
method = "xgbTree")
xgb_grid_2
xgb_trcontrol_2 = trainControl(method = "cv",
number = 7,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",
allowParallel = TRUE)
xgb_train_2 = train(x = as.matrix(lbase.cout.oh$train[,-c(1,2,3,length(lbase.cout.oh$full[1,]))]),
y = lbase.cout.oh$train[,'MeanClaimAmount'],
trControl = xgb_trcontrol_2,
tuneGrid = xgb_grid_2,
method = "xgbTree")
xgb_grid_2 = expand.grid(nrounds = seq(from = 1, to = 1001, by = 5))
xgb_trcontrol_2 = trainControl(method = "cv",
number = 1,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",
allowParallel = TRUE)
xgb_train_2 = train(x = as.matrix(lbase.cout.oh$train[,-c(1,2,3,length(lbase.cout.oh$full[1,]))])
, y = lbase.cout.oh$train[,'MeanClaimAmount']
, trControl = xgb_trcontrol_2
, tuneGrid = xgb_grid_2
, max_depth = xgb_train_1$bestTune$max_depth
, eta = xgb_train_1$bestTune$eta
, gamma = xgb_train_1$bestTune$gamma
, colsample_bytree = xgb_train_1$bestTune$colsample_bytree
, min_child_weight = xgb_train_1$bestTune$min_child_weight
, subsample = xgb_train_1$bestTune$subsample
, method = "xgbTree")
xgb_trcontrol_2 = trainControl(method = "cv",
number = 7,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",
allowParallel = TRUE)
xgb_grid_2 = expand.grid(nrounds = seq(from = 1, to = 1001, by = 5)
, max_depth = xgb_train_1$bestTune$max_depth
, eta = xgb_train_1$bestTune$eta
, gamma = xgb_train_1$bestTune$gamma
, colsample_bytree = xgb_train_1$bestTune$colsample_bytree
, min_child_weight = xgb_train_1$bestTune$min_child_weight
, subsample = xgb_train_1$bestTune$subsample
)
xgb_grid_2
xgb_trcontrol_2 = trainControl(method = "cv",
number = 7,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",
allowParallel = TRUE)
xgb_train_2 = train(x = as.matrix(lbase.cout.oh$train[,-c(1,2,3,length(lbase.cout.oh$full[1,]))]),
y = lbase.cout.oh$train[,'MeanClaimAmount'],
trControl = xgb_trcontrol_2,
tuneGrid = xgb_grid_2,
method = "xgbTree")
xgb_train_2$modelInfo$grid()
xgb_train_2$modelInfo$grid
xgb_train_2$levels
xgb_train_2
xgb_train_2$results
plot(x = xgb_train_2$results$nrounds, y = xgb_train_2$results$RMSE, type = "l")
plot(x = xgb_train_2$results$nrounds[which(xgb_train_2$results$RMSE<=1900),], y = xgb_train_2$results$RMSE[which(xgb_train_2$results$RMSE<=1900),], type = "l")
xgb_train_2$results$RMSE[which(xgb_train_2$results$RMSE<=1900),]
plot(x = xgb_train_2$results$nrounds[which(xgb_train_2$results$RMSE<=1900),]
, y = xgb_train_2$results$RMSE[which(xgb_train_2$results$RMSE<=1900)]
, type = "l")
xgb_train_2$results$RMSE<=1900
xgb_train_2$results$RMSE[,which(xgb_train_2$results$RMSE<=1900)
]
xgb_train_2$results$RMSE[which(xgb_train_2$results$RMSE<=1900)]
plot(x = xgb_train_2$results$nrounds[which(xgb_train_2$results$RMSE<=1900)]
, y = xgb_train_2$results$RMSE[which(xgb_train_2$results$RMSE<=1900)]
, type = "l")
plot(x = xgb_train_2$results$nrounds[which(xgb_train_2$results$RMSE<=1800)]
, y = xgb_train_2$results$RMSE[which(xgb_train_2$results$RMSE<=1800)]
, type = "l")
abline(v=xgb_train_2$bestTune$nrounds)
abline(v=xgb_train_2$bestTune$nrounds, col = "red")
plot(x = xgb_train_2$results$nrounds[which(xgb_train_2$results$RMSE<=1800)]
, y = xgb_train_2$results$RMSE[which(xgb_train_2$results$RMSE<=1800)]
, type = "l"
, xlab = "Nombre d'arbres"
,ylab = "RMSE"
)
abline(v=xgb_train_2$bestTune$nrounds, col = "red")
plot(x = xgb_train_2$results$nrounds[which(xgb_train_2$results$RMSE<=1800)]
, y = xgb_train_2$results$RMSE[which(xgb_train_2$results$RMSE<=1800)]
, type = "l"
, xlab = "Nombre d'arbres"
, ylab = "RMSE"
, main = "RMSE en fonction du nombre d'arbres"
)
abline(v=xgb_train_2$bestTune$nrounds, col = "red")
?abline
plot(x = xgb_train_2$results$nrounds[which(xgb_train_2$results$RMSE<=1800)]
, y = xgb_train_2$results$RMSE[which(xgb_train_2$results$RMSE<=1800)]
, type = "l"
, xlab = "Nombre d'arbres"
, ylab = "RMSE"
, main = "RMSE en fonction du nombre d'arbres"
)
abline(h = xgb_train_2$results$RMSE, col = "red")
plot(x = xgb_train_2$results$nrounds[which(xgb_train_2$results$RMSE<=1800)]
, y = xgb_train_2$results$RMSE[which(xgb_train_2$results$RMSE<=1800)]
, type = "l"
, xlab = "Nombre d'arbres"
, ylab = "RMSE"
, main = "RMSE en fonction du nombre d'arbres"
)
abline(h = xgb_train_2$bestTune$RMSE, col = "red")
xgb_train_2$bestTune$RMSE
xgb_train_2$bestTune$nrounds
abline(h = xgb_train_2$bestTune$RMSE[which(xgb_train_2$results$nrounds == xgb_train_2$bestTune$nrounds)], col = "red")
which(xgb_train_2$results$nrounds == xgb_train_2$bestTune$nrounds)
xgb_train_2$bestTune$RMSE[which(xgb_train_2$results$nrounds == xgb_train_2$bestTune$nrounds)]
View(xgb_train_2$results$nrounds)
View(xgb_train_2$results$RMSE)
xgb_train_2$results$RMSE[which(xgb_train_2$results$nrounds == xgb_train_2$bestTune$nrounds)]
abline(h = xgb_train_2$results$RMSE[which(xgb_train_2$results$nrounds == xgb_train_2$bestTune$nrounds)], col = "red")
abline(v=xgb_train_2$bestTune$nrounds, col = "red")
xgb_train_2$pred
xgb_train_2$times
xgb_train_2$finalModel
# Fichier pour entreposer le code hors du markdown. C'est plus pratique pour la partie algo
# On scinde en autant de partie que de méthodes à mettre en oeuvre
#Initialisation des packages ----
rm(list =ls())
install.packages("neuralnet")
library(neuralnet)
library(car)
library(caret)
library(nnet)
library(e1071)
library("xts")
library("sp")
library("zoo")
library(MASS)
library(magrittr)
library(rpart)
library(rpart.plot)
install.packages("CASdatasets", repos = "http://dutangc.free.fr/pub/RRepos/", type="source")
library("CASdatasets")
data("freMTPL2freq")
data("freMTPL2sev")
frequence <- freMTPL2freq
severite <- freMTPL2sev
rm(freMTPL2freq, freMTPL2sev)
#Mise en forme des donnéees ----
#On formate les données de la base de frequence
#frequence$ClaimNb <- frequence$ClaimNb %>% unname() %>% as.numeric()
frequence$VehPower <- as.factor(frequence$VehPower)
frequence$Exposure <- as.double(frequence$Exposure)
frequence$Area <- as.factor(frequence$Area)
frequence$VehAge <- as.integer(frequence$VehAge)
frequence$DrivAge <- as.integer(frequence$DrivAge)
frequence$BonusMalus <- as.integer(frequence$BonusMalus)
frequence$VehBrand <- as.factor(frequence$VehBrand)
frequence$VehGas <- as.factor(frequence$VehGas)
frequence$Region <- as.factor(frequence$Region)
frequence$ClaimNb<- as.numeric(frequence$ClaimNb)
frequence$Density=as.numeric(frequence$Density)
nrow(frequence)#678 010
#On formate la base de cout
severite$IDpol <- as.integer(severite$IDpol)
severite$ClaimAmount <- as.numeric(severite$ClaimAmount)
#On prend que les exposures inférieures à 1 ?
frequence <- frequence[frequence$Exposure <= 1,]
nrow(frequence)# 676 789
#Nos bases
head(frequence)
head(severite)
severite.mean <- aggregate(ClaimAmount ~ IDpol, data = severite, mean)
names(severite.mean) <- c("IDpol", "MeanClaimAmount")
base.mean <- merge(x = frequence, y = severite.mean, by = "IDpol", all.x = T)
base.mean$MeanClaimAmount <- replace(base.mean$MeanClaimAmount, is.na(base.mean$MeanClaimAmount), 0)
head(base.mean,5)
#On enlève les polices sinistrées avec un montant moyen de sinistres nul
base.mean <- base.mean[-which(base.mean$ClaimNb > 0 & base.mean$MeanClaimAmount ==0),]
nrow(base.mean)#667 673
summary(base.mean)
# Create DAta partition: Base apprentissage et Base Test ( Caret package) un problème avec le package!
#j'ai réparti la base avec la méthode traditionnelle
#75% de data en apprentissage et 25% test
#trainIndex <- createDataPartition(base.mean, p=.75, list=F)
#train <- base.mean[trainIndex, ]
#test <- base.mean[-trainIndex, ]
#size <- floor(0.75 * nrow(base.mean))
size <- floor(0.01 * nrow(base.mean)) #pour faire plus rapide
set.seed(100)
train_ind <- sample(seq_len(nrow(base.mean)), size =size)
train <- base.mean[train_ind, ]
test <- base.mean[-train_ind, ]
summary(train)
summary(test)
nrow(test)+nrow(train);nrow(base.mean)
# Avec décomposition
# Dans cet algo, on cherchera à détermine le paramètre  le plus important:le nombre de neurones
#sur la couche cachée parallèlement aux conditions d’apprentissage (temps ou nombre de boucles)
# A noter que l'alternative pour déterminer le nombre de neurones est celle du decay: paramètre de régularisation
#Mod?le de fréquence
# R?gression
# On Fitte un neural network avec la base d'apprentissage: La meilleure méhotde pour déterminer le nombre de layers
#et le nombre de neurones
n <- names(train)
mygrid <- expand.grid(size=c(1,2,3,4,5,6,7,8),decay=seq(1,5),KEEP.OUT.ATTRS = TRUE, stringsAsFactors = TRUE)
#as.formula nous permet de voir au plus clair sur les variables prise en compte pour les fit
varb <- as.formula(paste(" ClaimNb", paste(c("DrivAge","VehAge","VehPower","VehBrand","VehGas","BonusMalus","Area","Region","Density"), collapse = " + "),sep=" ~ "))
# Trainning net avec les poids
#train$ClaimNb = as.factor(train$ClaimNb) sur stackoverflow.com, on recommande que le y doit ?tre un factor mais n'emp?che
str(train) # v?rific des types de variables avant le fit
ctrl    <- trainControl(method = "cv",
number = 10,
savePredictions = TRUE,
verboseIter = T,
returnResamp = "all")
train.fit = train(varb ,data=train,method = "nnet",tuneGrid = mygrid ,trace=F, trControl =ctrl, weights=train$Exposure)
nnet=tune.nnet(varb,data = train, weights =train$Exposure, size=seq(1,7), decay=seq(1,5), maxit=1000,linout=TRUE)
nnet=tune.nnet(varb,data = train, weights =train$Exposure, size=seq(1,7), decay=seq(1,5), maxit=1000,linout=TRUE)
library(e1071)
install.packages("e1071")
